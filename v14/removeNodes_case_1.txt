If two worker node machines in a Kubernetes cluster (with one master node and three worker nodes) are physically stopped (e.g., powered off or failed), the cluster will detect them as `NotReady`, and you’ll need to take steps to ensure the remaining master node and one worker node remain in a working state, mimicking the outcome of a graceful cordon, drain, delete, and deprovision process. Below are the steps to achieve this, ensuring the cluster operates stably with one master and one worker node.

### Steps to Ensure the Cluster is in a Working State

1. **Verify Node Status**:
   - Check the current state of the cluster to confirm which nodes are affected:
     ```bash
     kubectl get nodes
     ```
   - The two stopped worker nodes (e.g., `worker-node-2` and `worker-node-3`) will appear as `NotReady`. The master node and the remaining worker node (e.g., `worker-node-1`) should be `Ready`.

2. **Check Pod Status**:
   - Identify pods that were running on the stopped nodes:
     ```bash
     kubectl get pods --all-namespaces -o wide
     ```
   - Pods on `worker-node-2` and `worker-node-3` may be in `Pending`, `Terminating`, or `Unknown` states, as Kubernetes cannot communicate with those nodes. Pods on `worker-node-1` and the master (if it schedules pods) should still be running.

3. **Reschedule Affected Pods**:
   - Kubernetes should automatically attempt to reschedule pods from the `NotReady` nodes to the remaining `Ready` worker node (`worker-node-1`), assuming the pods are managed by controllers like Deployments or ReplicaSets. To ensure this:
     - Verify that the remaining worker node has sufficient resources (CPU, memory, storage) to handle the relocated pods.
     - Check for any scheduling constraints (e.g., taints, tolerations, or node selectors):
       ```bash
       kubectl describe node worker-node-1
       ```
     - If the remaining node has taints preventing pod scheduling, remove them or add tolerations to the pods:
       ```bash
       kubectl taint nodes worker-node-1 key=value:NoSchedule-
       ```
   - If pods are stuck in `Pending`, check for issues like resource limits or PodDisruptionBudgets (PDBs) and resolve them (e.g., scale down replicas or adjust resource requests).

4. **Manually Delete Pods Stuck on Stopped Nodes**:
   - Pods on `NotReady` nodes may not terminate automatically. Force deletion if necessary:
     ```bash
     kubectl delete pod <pod-name> --namespace <namespace> --force --grace-period=0
     ```
   - Replace `<pod-name>` and `<namespace>` with the appropriate values. This allows the controller to recreate the pods on `worker-node-1`.

5. **Remove Stopped Nodes from the Cluster**:
   - Since the nodes are physically stopped, remove them from Kubernetes to clean up the cluster state:
     ```bash
     kubectl delete node worker-node-2
     kubectl delete node worker-node-3
     ```
   - This removes the nodes from the cluster’s inventory, ensuring Kubernetes no longer considers them for scheduling.

6. **Verify Cluster Health**:
   - Confirm that only the master node and `worker-node-1` remain:
     ```bash
     kubectl get nodes
     ```
   - Ensure all pods are running on `worker-node-1` (or the master, if it’s configured to run pods):
     ```bash
     kubectl get pods --all-namespaces -o wide
     ```
   - Check for any errors in the cluster:
     ```bash
     kubectl get events --all-namespaces
     ```

7. **Ensure Master Node Stability**:
   - Verify the master node is healthy and running critical control plane components (e.g., API server, scheduler, controller manager):
     ```bash
     kubectl get pods -n kube-system
     ```
   - If the master node is tainted to prevent workload scheduling (common in single-master setups), ensure it remains untainted only if it needs to run pods:
     ```bash
     kubectl describe node master-node | grep Taint
     ```
   - If the master is running pods and shouldn’t, apply a taint to restrict it to control plane tasks:
     ```bash
     kubectl taint nodes master-node node-role.kubernetes.io/control-plane:NoSchedule
     ```

8. **Handle Persistent Storage (if applicable)**:
   - If pods on the stopped nodes used PersistentVolumes (PVs), ensure the PersistentVolumeClaims (PVCs) are accessible on `worker-node-1`. For cloud-based storage or network-attached storage (e.g., NFS, EBS), PVCs should reattach automatically. Verify:
     ```bash
     kubectl get pvc --all-namespaces
     ```
   - If using local storage, data on the stopped nodes may be inaccessible. Plan for data recovery or replication if critical.

9. **Clean Up Residual Resources**:
   - Since the nodes were physically stopped, no deprovisioning is needed on the machines themselves. However, if using a cloud provider or cluster management tool (e.g., EKS, GKE, KOPS), update the infrastructure configuration to remove the stopped nodes from the node group or instance list to prevent auto-recreation.
   - If the nodes restart unexpectedly, stop the `kubelet` service on them to prevent rejoining:
     ```bash
     sudo systemctl stop kubelet
     ```

10. **Monitor and Validate**:
    - Ensure the cluster is stable with only the master and one worker node:
      ```bash
      kubectl cluster-info
      kubectl get componentstatuses
      ```
    - Monitor resource usage on `worker-node-1` to ensure it’s not overloaded:
      ```bash
      kubectl top nodes
      kubectl top pods
      ```
    - If resource constraints arise, consider scaling up `worker-node-1` or adding a new worker node.

### Achieving a Similar State to Graceful Removal
The goal is to replicate the state after a graceful cordon, drain, delete, and deprovision:
- **No pods on stopped nodes**: Achieved by deleting stuck pods and letting Kubernetes reschedule them.
- **Nodes removed from cluster**: Achieved by deleting the nodes with `kubectl delete node`.
- **Workloads on remaining node**: Ensured by verifying pod rescheduling and resource availability.
- **Clean cluster state**: Achieved by removing references to stopped nodes and validating cluster health.

### Key Considerations
- **Resource Capacity**: The remaining worker node must have enough resources to handle all pods. If not, scale up or reduce replicas of non-critical workloads.
- **Stateful Applications**: Ensure StatefulSets or databases have their data reattached or replicated. Manual intervention may be needed for local storage.
- **PodDisruptionBudgets (PDBs)**: Check if PDBs prevent pod evictions and adjust if necessary.
- **Cluster Autoscaler**: If enabled, update its configuration to reflect the reduced node count.
- **High Availability**: With only one worker node, the cluster has reduced redundancy. Consider adding another worker node if HA is critical.

If you encounter specific issues (e.g., stuck pods, resource errors, or storage problems), provide details, and I can guide you further!
